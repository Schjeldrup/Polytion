{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyGAN layers\n",
    "This notebook contains the different polynomial approximation layers for us in the generator structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- pytorch version of skimage transform: use nn.Upscale!\n",
    "- Exchange lists for tensors: done for CP, done for FTT\n",
    "- Tests: is threading overhead really worth it? Removed one thread level in CP for better performance and reversed thread queue, nothing had to change for FFT\n",
    "\n",
    "- Implement batch versions of layers: done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import skimage\n",
    "import threading\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the desired order and rank of the following algorithms here:\n",
    "N = 5\n",
    "rank = 4\n",
    "scalefactor = 4\n",
    "\n",
    "batchsize = 8\n",
    "ch = 1 # Grayscale images\n",
    "\n",
    "# Save a standard set of inputs\n",
    "imwidth, imheight = 512, 512\n",
    "lrwidth, lrheight = int(imwidth/scalefactor), int(imheight/scalefactor)\n",
    "hi_res_sample = torch.rand(imwidth, imheight).float()\n",
    "lo_res_sample  = torch.rand(lrwidth, lrheight).float()\n",
    "\n",
    "hi_res_batch = torch.rand(batchsize, ch, imwidth, imheight).float()\n",
    "lo_res_batch = torch.rand(batchsize, ch, lrwidth, lrheight).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PolyGAN CP decomposition\n",
    "Here we implement the basic CP for the polyGAN polynomial approximation, Eq (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is given by\n",
    "\\begin{align*}\n",
    "G(z)&=\\sum_{n=1}^{N}\\left(\\mathcal{W}^{[n]} \\prod_{k=2}^{n+1} \\times_{k} z\\right)+\\boldsymbol{\\beta} \\\\\n",
    "&=\\sum_{n=1}^{N}\\left( \\sum_{r=1}^R \\alpha^{[1]}_{i_1, r} \\cdot \\prod_{k=2}^{n+1} \\left(\\sum_{i_k = 1}^{s} z_{i_k} \\cdot \\alpha^{[k]}_{i_k, r} \\right) \\right)+\\boldsymbol{\\beta}\n",
    "\\end{align*}\n",
    "with $\\boldsymbol{z} \\in \\mathcal{R}^{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyGAN_CP_Layer(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight, verbose = 0):\n",
    "        super(PolyGAN_CP_Layer, self).__init__()\n",
    "        \n",
    "        # N = order of the polynomial = order of the tensor A:\n",
    "        # A is of dimension (s, s, ..., s) = (N x s)\n",
    "        # rank = rank used for the tensor cores\n",
    "        # size = length of the flattened input\n",
    "        self.N = N\n",
    "        self.rank = rank\n",
    "        self.s = imwidth * imheight\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # bias and weights\n",
    "        b = torch.empty(self.s,1)\n",
    "        torch.nn.init.xavier_normal_(b)\n",
    "        self.b = torch.nn.Parameter(b.reshape(self.s))\n",
    "        \n",
    "        self.W = torch.nn.ParameterList()\n",
    "        self.shapelist = []\n",
    "        # 0th order: bias [s], 1st order: weight [s, s], 2nd order: weight [s, s, s], ...\n",
    "        for n in range(1, N + 1):\n",
    "            tshape = [self.s]*(n + 1)\n",
    "            self.shapelist.append(tshape)\n",
    "                        \n",
    "            # Here we allocate all the factor matrices (using CP):\n",
    "            for o in range(n + 1):\n",
    "                # We need one factor matrix per dimension of the weight matrix of that order.\n",
    "                # So 3rd order [d, d, d] has 3 factor matrices of shape [d, rank]\n",
    "                factor_matrix = torch.zeros((self.s, self.rank))\n",
    "                torch.nn.init.xavier_normal_(factor_matrix)\n",
    "                self.W.append(torch.nn.Parameter(factor_matrix))\n",
    "        \n",
    "        if self.verbose != 0:\n",
    "            print(\"self.shapelist =\")\n",
    "            for shape in self.shapelist:\n",
    "                print(shape)\n",
    "            print(\"self.N =\", self.N, \"?=\", len(self.shapelist), \"= len(self.shapelist)\") \n",
    "            print(\"W has\", len(self.W), \"elements of shape\", self.W[0].shape)\n",
    "                \n",
    "                \n",
    "    def parallelScalarProd(self, n, r):\n",
    "        f = 1\n",
    "        for k in range(1, n):\n",
    "            f *= torch.dot(self.z, self.W[k][:,r])\n",
    "        # The .data was necessary to remove gradient info, as the operation was listed as in-place\n",
    "        self.VecProds.data[n, r] = f\n",
    "        if self.verbose != 0:\n",
    "            logging.info('f = %f, is a single number', f)\n",
    "        return\n",
    "    \n",
    "    def parallelRankSum(self, n):\n",
    "        threads = []\n",
    "        for r in range(self.rank):\n",
    "            # Perform parallel computation of the products: tremendous speedup.\n",
    "            process = threading.Thread(target=self.parallelScalarProd, args=(n, r,))\n",
    "            process.start()\n",
    "            threads.append(process)  \n",
    "            \n",
    "        Rsum = torch.zeros(self.s)\n",
    "        for r in range(self.rank):\n",
    "            threads[r].join()\n",
    "            Rsum += self.W[0][:,r] * self.VecProds[n, r]\n",
    "        self.Rsums[n] = Rsum\n",
    "        if self.verbose != 0:\n",
    "            shapes = list(self.Rsums[n].shape)\n",
    "            logging.info('Size of Rsum[%d] = %d', n, shapes[0])\n",
    "        return \n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Compute the forward pass: the nmode multiplications f = self.b + self.b + self.W[0]*z + z.T*...\n",
    "        # We can also compute every rank computation in parallel and sum results.\n",
    "        # For every n, spawn a thread to compute one part of the Nsum, so Nsum = sum(Rsums).\n",
    "        # For every r, spawn a thread to compute one part of the Rsum, so Rsum = sum(a^1*VecProds)\n",
    "        self.z = z\n",
    "        threads = []\n",
    "        self.Rsums = torch.zeros(self.N, self.s) #[None]*self.N\n",
    "        self.VecProds = torch.zeros(self.N, self.rank) #[None]*self.rank*self.N\n",
    "        for n in range(self.N): \n",
    "            # Perform parallel computation of the rank summation: tremendous speedup.\n",
    "            process = threading.Thread(target=self.parallelRankSum, args=(n,))\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "\n",
    "        Nsum = torch.zeros(self.s)\n",
    "        for n in range(self.N-1, -1, -1):\n",
    "            threads[n-1].join()\n",
    "            Nsum += self.Rsums[n-1]\n",
    "        \n",
    "        return Nsum + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Tensor-train contraction\n",
    "See \"Parallelized Tensor Train Learning of Polynomial\n",
    "Classifiers\". Eq (8), (10), (11) and algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is given by\n",
    "\\begin{align*}\n",
    "\\mathcal{G_1} (i_1) \\cdot \\prod_{k=2}^{n+1} \\left(\\sum_{i_k = 1}^{s} z_{i_k} \\cdot \\mathcal{G_k}(i_k) \\right)\n",
    "\\end{align*}\n",
    "with $\\boldsymbol{z} \\in \\mathcal{R}^{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTT_Layer(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight, verbose = 0):\n",
    "        super(FTT_Layer, self).__init__()\n",
    "        \n",
    "        # N = order of the polynomial = order of the tensor A:\n",
    "        # A is of dimension (s, s, ..., s) = (N x s)\n",
    "        # rank = rank used for the tensor cores\n",
    "        # size = length of the flattened input\n",
    "        self.N = N\n",
    "        self.rank = rank\n",
    "        self.s = imwidth * imheight\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Make a list of TTcore ranks, starting from r_0 = r_N = 1: perhaps feed it in as a list? isinstance(rank, list)\n",
    "        self.ranklist = [1, 1]\n",
    "        for n in range(self.N - 1):\n",
    "            self.ranklist.insert(-1, self.rank)\n",
    "        \n",
    "        # Start by making the tensor train: store the matrices in one big parameterlist\n",
    "        self.TT = torch.nn.ParameterList()\n",
    "        \n",
    "        # Make s instances for every mode of the tensor, or make a 3D tensor instead:\n",
    "        for n in range(self.N):\n",
    "            # Make tensors of size (r_{k-1}, n_{k} = self.s, r_{k})\n",
    "            TTcore = torch.empty(self.ranklist[n], self.s, self.ranklist[n+1])\n",
    "            torch.nn.init.xavier_normal_(TTcore)\n",
    "            self.TT.append(torch.nn.Parameter(TTcore))\n",
    "                        \n",
    "        if self.verbose != 0:\n",
    "            print(\"self.ranklist =\", self.ranklist)\n",
    "            print(\"self.N =\", self.N, \"+ 1 ?=\", len(self.ranklist), \"= len(self.ranklist)\") \n",
    "            print(\"TT has\", len(self.TT), \"elements:\")\n",
    "            for i, tt in enumerate(self.TT):\n",
    "                print(\"element\", i, \":\", tt.shape)\n",
    "    \n",
    "    def test(self):\n",
    "        # The multiplication z_i * G_k(i) is really [1] * [rank, rank] (or for last cart [1] * [rank, 1] )\n",
    "        # Here we test whether the vectorized multiplication yields the same result. Warning: this is slow!\n",
    "        k = 1\n",
    "        # According to the algorithm:\n",
    "        summation = 0\n",
    "        for i in range(self.s):\n",
    "            summation += self.z[i] * self.TT[k][:,i,:]\n",
    "        \n",
    "        # Naive fast implmentation is: self.V[k] = self.z @ self.TT[k]. Does not yield same results.\n",
    "        # Instead, permute the axes of the train cart from [rank, s, rank] to [s, rank**2] (or [s, rank*1])\n",
    "        perm = self.TT[k].permute(1,0,2).reshape(self.s,-1)\n",
    "        product = torch.matmul(self.z, perm).reshape(self.ranklist[k], self.ranklist[k+1])\n",
    "        \n",
    "        # Assert whether they are the same:\n",
    "        print(\"Are the two methods equivalent?\", torch.allclose(summation, product, rtol = 1e-03, atol = 1e-03))\n",
    "        return\n",
    "    \n",
    "    def parallelVecProd(self, k):\n",
    "        d1, d2 = self.ranklist[k], self.ranklist[k+1]\n",
    "        tmp = torch.matmul(self.z, self.TT[k].permute(1,0,2).reshape(self.s, d1*d2))\n",
    "        self.V.data[k,0:d1,0:d2] = tmp.reshape(d1, d2)\n",
    "        return\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Compute the forward pass: the nmode multiplications f = A x1 z x2 z x3 ··· x(N-1) z\n",
    "        # Follow algorithm 1: allocate space and compute each V^(k), possible in parallel with threads.\n",
    "        # V^(0) will not be used as it is unnecessary to compute it, it's just self.TT[0][0,:,:] \n",
    "        self.V = torch.zeros(self.N, self.rank, self.rank) #[None] * self.N\n",
    "        self.z = z\n",
    "        threads = []\n",
    "        \n",
    "        if self.verbose != 0:\n",
    "            print(\"Threads are computing..\")\n",
    "            self.test()\n",
    "            \n",
    "        for k in range(1, self.N):\n",
    "            # Perform parallel computation of the products: tremendous speedup. See test() for more info.\n",
    "            process = threading.Thread(target=self.parallelVecProd, args=(k,))\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "            \n",
    "        # Start the whole product chain now, so that we have [(1),s,r] x [r, r] x ... x [r, (1)] = s\n",
    "        f = self.TT[0][0,:,:] \n",
    "        for k in range(1, self.N):\n",
    "            d1, d2 = self.ranklist[k], self.ranklist[k+1]\n",
    "            threads[k-1].join()\n",
    "            f @= self.V[k,0:d1,0:d2]\n",
    "        #logging.info('%d, %d', d1, d2)\n",
    "        return f.reshape(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug and play\n",
    "Test the different layers in a standard net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch output has shape torch.Size([8, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# define testnetwork\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, layer, N, rank, imwidth, imheight, scalefactor):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.imwidth, self.imheight = imwidth, imheight\n",
    "        self.s = imwidth*imheight\n",
    "        self.PolyLayer = layer(N, rank, imwidth, imheight, 0)\n",
    "        # Register the scalefactor of the upscaling procedure in the forward pass of the generator.\n",
    "        self.scalefactor = scalefactor\n",
    "        \n",
    "    def BatchInParallel(self, b):\n",
    "        # self.c-1 because for one channel we want to access index 0\n",
    "        self.x[b, self.c-1, :] = self.PolyLayer(self.x[b, self.c-1, :])\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Register x as attribute for parallel access\n",
    "        self.bsize, self.c, w, h = x.shape\n",
    "        self.x = x\n",
    "        \n",
    "        # Pytorch implementation = better:\n",
    "        upscale = torch.nn.Upsample(scale_factor=self.scalefactor, mode='bilinear', align_corners=False)\n",
    "        self.x = upscale(self.x)\n",
    "        \n",
    "        self.x = self.x.reshape(self.bsize, self.c, self.s) # flatten to the 1D equivalent vector\n",
    "        \n",
    "        # Start threads\n",
    "        threads = []\n",
    "        for batch in range(self.bsize):\n",
    "            # Perform parallel computation of the rank summation: tremendous speedup.\n",
    "            process = threading.Thread(target=self.BatchInParallel, args=(batch,))\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "        \n",
    "        # Wait for threads to end:\n",
    "        for batch in range(self.bsize):\n",
    "            threads[batch].join()\n",
    "        \n",
    "        #self.x = self.PolyLayer(self.x)\n",
    "        self.x = self.x.reshape(self.bsize, self.c, self.imwidth, self.imheight)\n",
    "        return self.x\n",
    "\n",
    "#net = Generator(FTT_Layer, N, rank, imwidth, imheight, scalefactor)\n",
    "net = Generator(PolyGAN_CP_Layer, N, rank, imwidth, imheight, scalefactor)\n",
    "\n",
    "# Make batch version:\n",
    "output = net(lo_res_batch)\n",
    "print(\"batch output has shape\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Time the learning process:\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loops = 10\n",
    "\n",
    "net.train()\n",
    "\n",
    "pred_timer = []\n",
    "crit_timer = []\n",
    "optm_timer = []\n",
    "loss_timer = []\n",
    "step_timer = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for t in tqdm(range(loops)):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    pred_start = time.time()\n",
    "    pred = net(lo_res_batch)\n",
    "    pred_timer.append(time.time() - pred_start)\n",
    "    \n",
    "    crit_start = time.time()\n",
    "    loss = criterion(pred, hi_res_batch)\n",
    "    crit_timer.append(time.time() - crit_start)\n",
    "    \n",
    "    optm_start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    optm_timer.append(time.time() - optm_start)\n",
    "    \n",
    "    loss_start = time.time()\n",
    "    loss.backward()\n",
    "    loss_timer.append(time.time() - loss_start)\n",
    "    \n",
    "    step_start = time.time()\n",
    "    optimizer.step()\n",
    "    step_timer.append(time.time() - step_start)\n",
    "\n",
    "total_elapsed = time.time() - start\n",
    "elapsed_per_loop = float(total_elapsed/loops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time = 14.037233829498291 s, with 1.403723382949829 s per loop\n",
      "pred_timer = 0.032852768898010254 s on average\n",
      "crit_timer = 0.000578424334526062 s on average\n",
      "optm_timer = 7.797777652740479e-05 s on average\n",
      "loss_timer = 0.1406235784292221 s on average\n",
      "step_timer = 0.0011578351259231567 s on average\n"
     ]
    }
   ],
   "source": [
    "mean = lambda x : sum(x)/len(x)\n",
    "\n",
    "print(\"Total elapsed time =\", total_elapsed, \"s, with\", elapsed_per_loop, \"s per loop\")\n",
    "print(\"pred_timer =\", mean(pred_timer)/batchsize, \"s on average\")\n",
    "print(\"crit_timer =\", mean(crit_timer)/batchsize, \"s on average\")\n",
    "print(\"optm_timer =\", mean(optm_timer)/batchsize, \"s on average\")\n",
    "print(\"loss_timer =\", mean(loss_timer)/batchsize, \"s on average\")\n",
    "print(\"step_timer =\", mean(step_timer)/batchsize, \"s on average\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
