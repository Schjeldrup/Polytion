{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring tensor decompositions in pytorch\n",
    "Simon Aertssen\n",
    "12/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an exploration of the different functions and methods necessary to perform the CANDECOMP/PARAFAC and Tensor-Train Decompositions. These will be gathered in a ```polution``` package later. The ```tensorly```package is a great resource: https://github.com/tensorly/tensorly/tree/master/tensorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Useful products:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{Matrix Kronecker product}$ is defined as\n",
    "\n",
    "\\begin{align*} \\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}} \\otimes \\boldsymbol{Q}^{\\:\\mathrm{K} \\times \\mathrm{L}}=\\boldsymbol{R}^{\\:\\mathrm{IK} \\times \\mathrm{JL}}, \\text { such that } r_{k+K(i-1), \\: l+L(j-1)}=p_{i j} q_{k l} \\end{align*}\n",
    "\n",
    "Naive: use loops. Smart: reshape P and Q so that their dimensions do not overlap, then the elementwise or Hadamard product gives the desired result.\n",
    "\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product) for the examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KronProd2D(P, Q):\n",
    "    # Register dimensions.\n",
    "    I, J = P.shape\n",
    "    K, L = Q.shape\n",
    "    \n",
    "    # Adjust dimensions of P and Q to perform smart multiplication:\n",
    "    # interweave the dimensions containing values and perform elementwise multiplication.\n",
    "    P = P.view(I, 1, J, 1)\n",
    "    Q = Q.view(1, K, 1, L)\n",
    "    \n",
    "    R = P * Q\n",
    "    return R.view(I*K, J*L)\n",
    "\n",
    "def KronProd(P, Q):\n",
    "    # This should work for higher order tensors.\n",
    "    # Register and check dimensions.\n",
    "    pshape = P.shape\n",
    "    qshape = Q.shape\n",
    "    if P.dim() != Q.dim():\n",
    "        raise ValueError('Matrices should be of the same order: P.dim() =' + str(P.dim()) + ' != Q.dim() = ' + str(Q.dim()))\n",
    "    \n",
    "    # Adjust dimensions of P and Q to perform smart multiplication:\n",
    "    # interweave the dimensions containing values and perform elementwise multiplication.\n",
    "    # Start with a list of ones and set dimensions as every even or uneven index.\n",
    "    pindices = [1]*2*len(pshape)    \n",
    "    pindices[::2] = pshape\n",
    "    qindices = [1]*2*len(qshape)\n",
    "    qindices[1::2] = qshape\n",
    "        \n",
    "    P = P.view(pindices)\n",
    "    Q = Q.view(qindices)\n",
    "    \n",
    "    R = P * Q\n",
    "    rshape = [p*q for p, q in zip(pshape,qshape)]\n",
    "    return R.view(rshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.tensor([[1, -4, 7], [-2, 3, 3]])\n",
    "Q = torch.tensor([[8, -9, -6, 5], [1, -3, -4, 7], [2, 8, -8, -3], [1, 2, -5, -1]])\n",
    "R = KronProd(P, Q)\n",
    "assert torch.all(R.eq(KronProd2D(P, Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{Kahtri-Rhao product}$ is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}}|\\otimes| \\boldsymbol{Q}^{\\mathrm{K} \\times \\mathrm{J}}=\\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}} \\odot \\boldsymbol{Q}^{\\mathrm{K} \\times \\mathrm{J}}=\\boldsymbol{R}^{\\:\\mathrm{IK} \\times \\mathrm{J}}, \\text { such that } r_{k+K(i-1), j}=p_{i j} q_{k j}\n",
    "\\end{align*}\n",
    "\n",
    "This can be seen as a column-wise kronecker product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KathRaoProd(P, Q):\n",
    "    # Register and check dimensions\n",
    "    pshape = P.shape\n",
    "    qshape = Q.shape\n",
    "    J = pshape[-1]\n",
    "    if J != qshape[-1]:\n",
    "        raise ValueError('Matrices should have the same number of columns: ' + str(J) + ' != ' + str(qshape[-1]))\n",
    "    \n",
    "    # Make R an empty tensor\n",
    "    rshape = [p*q for p, q in zip(pshape,qshape)]\n",
    "    rshape[-1] = J\n",
    "    R = torch.zeros(rshape)\n",
    "    for j in range(J):\n",
    "        R[:,j] = KronProd(P[:,j], Q[:,j])\n",
    "                \n",
    "    # Tried with slicing but did not seem to work:\n",
    "    # column_indices = list(range(J)) or slice(0,J)\n",
    "    # R[:, column_indices] = KronProd(P[:, column_indices], Q[:, column_indices])\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "Q = torch.tensor([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n",
    "R = KathRaoProd(P, Q)\n",
    "#print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mode-m Matricization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the decompositions we will need tensor unfolding. The mode-$n$ matricization of a tensor $\\mathcal{X} \\in \\mathrm{R}^{I_{1} \\times I_{2} \\times \\ldots \\times I_{N}}$ is denoted $\\boldsymbol{X}_{(n)}$ and arranges the mode-$n$ fibres to be the columns of the resulting matrix.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{X}^{I_{1} \\times I_{2} \\times \\ldots \\times I_{N}} \\hspace{5mm}\n",
    "\\overset{\\underset{\\text{unfolding}}{\\longrightarrow}}{\\underset{\\text{folding}}{\\longleftarrow}} \\hspace{5mm}\n",
    "\\boldsymbol{X}_{(n)}^{I_{n} \\times I_{1} \\cdot I_{2} \\cdots I_{n-1} \\cdot I_{n+1} \\cdots I_{N}} \n",
    "\\end{align*}\n",
    "\n",
    "While writing the function underneath (```def Unfold```) it became clear that Kolda's definition was lacking some of the intuition behind torch tensors and their indexing. A good discussion can be found here: https://jeankossaifi.com/blog/unfolding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unfold(tensor, mode):\n",
    "    # Notation sets minimal mode to be 1, but we want to map that to zero. We also cannot mode_n unfold a matrix with n-1 modes.\n",
    "    if mode > len(tensor.shape) or mode < 0:\n",
    "        raise ValueError('Tensor has order ' + str(len(tensor.shape)) + ', cannot give mode ' + str(mode))\n",
    "    \n",
    "    # Register tensor shape and mode size\n",
    "    order = tensor.dim()\n",
    "    tshape = tensor.shape    \n",
    "    I_n = tshape[mode]\n",
    "    \n",
    "    # To get the prescribed matrix in Kolda et al, we need to perform a permutation of the axes.\n",
    "    # The selected mode needs to be viewed as the first, and all other dimensions should reverse their order.\n",
    "    # It took a lot of experiments to get this right.\n",
    "    axes = list(range(order))\n",
    "    axes.pop(mode)\n",
    "    axes = axes[::-1]\n",
    "    axes.insert(0, mode)\n",
    "    \n",
    "    return tensor.permute(*axes).reshape(I_n, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([3, 4, 2])\n",
      "X_1 = \n",
      "[[ 1  4  7 10]\n",
      " [ 2  5  8 11]\n",
      " [ 3  6  9 12]]\n",
      "X_2 = \n",
      "[[13 16 19 22]\n",
      " [14 17 20 23]\n",
      " [15 18 21 24]]\n"
     ]
    }
   ],
   "source": [
    "# Following the example on p460:\n",
    "# This definition of X gives wrong dimensions:\n",
    "#X = torch.tensor([[ [1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12] ], [ [13, 16, 19, 22], [14, 17, 20, 23], [15, 18, 21, 24] ]])\n",
    "\n",
    "X = torch.tensor([[ [1,13], [4,16], [7,19], [10,22] ], [ [2,14], [5,17], [8,20], [11,23] ], [ [3,15], [6,18], [9,21], [12,24] ]])\n",
    "print(\"X.shape = \" + str(X.shape))\n",
    "X1 = X[...,0]\n",
    "X2 = X[...,1]\n",
    "print(\"X_1 = \\n\" + str(X1.numpy()))\n",
    "print(\"X_2 = \\n\" + str(X2.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  7 10 13 16 19 22]\n",
      " [ 2  5  8 11 14 17 20 23]\n",
      " [ 3  6  9 12 15 18 21 24]]\n",
      "[[ 1  2  3 13 14 15]\n",
      " [ 4  5  6 16 17 18]\n",
      " [ 7  8  9 19 20 21]\n",
      " [10 11 12 22 23 24]]\n",
      "[[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      " [13 14 15 16 17 18 19 20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "print(Unfold(X, 0).numpy())\n",
    "print(Unfold(X, 1).numpy())\n",
    "print(Unfold(X, 2).numpy())\n",
    "# Note that the .numpy() is only here to represent the unfolded tensors in a nice format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need a ```Fold``` function? Update: yes. Perform reverse operations of Unfold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fold(unfolded_tensor, mode, shape):\n",
    "    if mode > len(shape) or mode < 0:\n",
    "        raise ValueError('Folded tensor has order ' + str(len(shape)) + ', cannot give mode ' + str(mode))\n",
    "    \n",
    "    shape = list(shape)\n",
    "    axis = shape.pop(mode)\n",
    "    shape = shape[::-1]\n",
    "    shape.insert(0, axis)\n",
    "    \n",
    "    return unfolded_tensor.reshape(shape).permute(*axes).reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  4  7 10]\n",
      "  [ 2  5  8 11]]\n",
      "\n",
      " [[ 3  6  9 12]\n",
      "  [13 16 19 22]]\n",
      "\n",
      " [[14 17 20 23]\n",
      "  [15 18 21 24]]]\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]\n",
      "\n",
      " [[13 14 15]\n",
      "  [16 17 18]]\n",
      "\n",
      " [[19 20 21]\n",
      "  [22 23 24]]]\n",
      "[[[ 1  2  3]\n",
      "  [13 14 15]\n",
      "  [ 4  5  6]\n",
      "  [16 17 18]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [19 20 21]\n",
      "  [10 11 12]\n",
      "  [22 23 24]]]\n"
     ]
    }
   ],
   "source": [
    "print(Fold(Unfold(X, 0), 0, X.shape).numpy())\n",
    "print(Fold(Unfold(X, 1), 1, X.shape).numpy())\n",
    "print(Fold(Unfold(X, 2), 2, X.shape).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n",
      "tensor([[[ 1, 13],\n",
      "         [ 4, 16],\n",
      "         [ 7, 19],\n",
      "         [10, 22]],\n",
      "\n",
      "        [[ 2, 14],\n",
      "         [ 5, 17],\n",
      "         [ 8, 20],\n",
      "         [11, 23]],\n",
      "\n",
      "        [[ 3, 15],\n",
      "         [ 6, 18],\n",
      "         [ 9, 21],\n",
      "         [12, 24]]])\n",
      "torch.Size([3, 2, 4])\n",
      "tensor([[[ 1,  4,  7, 10],\n",
      "         [ 2,  5,  8, 11]],\n",
      "\n",
      "        [[ 3,  6,  9, 12],\n",
      "         [13, 16, 19, 22]],\n",
      "\n",
      "        [[14, 17, 20, 23],\n",
      "         [15, 18, 21, 24]]])\n"
     ]
    }
   ],
   "source": [
    "# Let's be absolutely shure:\n",
    "print(X.shape)\n",
    "print(X)\n",
    "folded = Fold(Unfold(X, 0), 0, X.shape)\n",
    "print(folded.shape)\n",
    "print(folded)\n",
    "\n",
    "#assert torch.all(X.eq(Fold(Unfold(X, 0), 0, X.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CP decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the alternating least squares approach. Start with random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPD(tensor, rank=5, maxiter=25):\n",
    "    # Note that we will perform n_iter_max iterations per tensor order!\n",
    "    # Register input variables:\n",
    "    tshape = tensor.shape\n",
    "    order = tensor.dim()\n",
    "    \n",
    "    # Initialization: create a list of matrices to represent the input tensor: \n",
    "    # tensor = [t_1, t_2, .. , t_order]\n",
    "    # Incorrect: ABC = [torch.rand((rank, rank))]*order\n",
    "    ABC = []\n",
    "    for o in range(order):\n",
    "        ABC.append(torch.rand((tshape[o], rank)))\n",
    "    ABC_indices = list(range(order))\n",
    "    \n",
    "    # Main loop: for every iteration, adjust every factor matrix\n",
    "    for iteration in range(maxiter):\n",
    "        for i, fmatrix in enumerate(ABC):\n",
    "            # Get all the other fmatrices, temporarily remove the element:\n",
    "            ABC.pop(i)\n",
    "\n",
    "            # Construct V: use the first element ABC[0] and the loop over everything after with ABC[1::]\n",
    "            V = ABC[0].t() @ ABC[0]\n",
    "            for ABC_notfmatrix in ABC[1::]:\n",
    "                V = V * (ABC_notfmatrix.t() @ ABC_notfmatrix)\n",
    "            \n",
    "            # Construct W (kr product of all matrices other way around):\n",
    "            # take the last element of ABC and then loop over ABC[::-1] excluding the last element.\n",
    "            # Example: ABC = [1,2,3,4] and ABC[::-1][1::] = [3,2,1] and ABC[1::-1] = [2,1]\n",
    "            W = ABC[-1]\n",
    "            for ABC_notfmatrix in ABC[::-1][1::]:\n",
    "                #print(ABC_notfmatrix)\n",
    "                #print(W.shape)\n",
    "                #print(ABC_notfmatrix.shape)\n",
    "                W = KathRaoProd(W, ABC_notfmatrix)\n",
    "            #print(Unfold(tensor, i).shape)\n",
    "            #print(W.shape)\n",
    "            #print(torch.pinverse(V).shape)\n",
    "            #print(i)\n",
    "            fmatrix = Unfold(tensor, i) @ W @ torch.pinverse(V)\n",
    "            \n",
    "            # Push the lost fmatrix back in:\n",
    "            ABC.insert(i, fmatrix)\n",
    "    return ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# test:\n",
    "tensor = torch.rand((10,15,20))\n",
    "factor_matrices = CPD(tensor)\n",
    "print(len(factor_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "vector =  torch.Size([15])\n",
      "torch.Size([10, 15])\n",
      "vector =  torch.Size([20])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vector and vector expected, got 2D, 1D tensors at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensorMath.cpp:886",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-777-4aaed7188be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#    tensor +=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-777-4aaed7188be1>\u001b[0m in \u001b[0;36mCompose\u001b[0;34m(factors)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vector = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mouter_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_prod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_prod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#tmp = torch.ger(factors[r][:,1].unsqueeze(2), factors[0][:,1].unsqueeze())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vector and vector expected, got 2D, 1D tensors at /Users/distiller/project/conda/conda-bld/pytorch_1565272526878/work/aten/src/TH/generic/THTensorMath.cpp:886"
     ]
    }
   ],
   "source": [
    "def Compose(factors):\n",
    "    # Register dimensions of the tensor:\n",
    "    order = len(factors)\n",
    "    tshape = [x.shape[0] for x in factors]\n",
    "    rank = factors[0].shape[-1]\n",
    "    tensor = torch.zeros(tshape)\n",
    "    \n",
    "    #tmp_vecs = [x[:,1] for x in factors]\n",
    "    \n",
    "    for r in range(rank):\n",
    "        tmp_vecs = [x[:,r] for x in factors]\n",
    "        outer_prod = tmp_vecs[0]\n",
    "        print(outer_prod.shape)\n",
    "        for vector in tmp_vecs[1::]:\n",
    "            print(\"vector = \", vector.shape)\n",
    "            outer_prod = torch.ger(outer_prod, vector)\n",
    "            print(outer_prod.shape)\n",
    "        #tmp = torch.ger(factors[r][:,1].unsqueeze(2), factors[0][:,1].unsqueeze())\n",
    "    #    tensor += \n",
    "    \n",
    "Compose(factor_matrices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "torch.Size([5, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones((5,3))\n",
    "x = torch.zeros((5,8))\n",
    "bmm = torch.bmm(x.unsqueeze(2), y.unsqueeze(1))\n",
    "print(bmm.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
