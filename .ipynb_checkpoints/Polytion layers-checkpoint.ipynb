{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyGAN layers\n",
    "This notebook contains the different polynomial approximation layers for us in the generator structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the desired order and rank of the following algorithms here:\n",
    "N = 5\n",
    "rank = 3\n",
    "\n",
    "# Save a standard set of inputs\n",
    "imwidth, imheight = 512, 512\n",
    "high_res_sample = torch.rand(imwidth,imwidth).float()\n",
    "low_res_sample  = torch.rand(int(imwidth/4), int(imwidth/4)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Tensor-train contraction\n",
    "See \"Parallelized Tensor Train Learning of Polynomial\n",
    "Classifiers\". Eq (8), (10), (11) and algorithm 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class FTT_Layer(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight, verbose = 0):\n",
    "        super(FTT_Layer, self).__init__()\n",
    "        \n",
    "        # N = order of the polynomial = order of the tensor A:\n",
    "        # A is of dimension (s, s, ..., s) = (N x s)\n",
    "        # rank = rank used for the tensor cores\n",
    "        # size = length of the flattened input\n",
    "        self.N = N\n",
    "        self.rank = rank\n",
    "        self.s = imwidth * imheight\n",
    "        \n",
    "        # Make a list of TTcore ranks, starting from r_0 = r_N = 1: perhaps feed it in as a list? isinstance(rank, list)\n",
    "        self.ranklist = [1, 1]\n",
    "        for n in range(self.N - 1):\n",
    "            self.ranklist.insert(-1, self.rank)\n",
    "        \n",
    "        # Start by making the tensor train: store the matrices in one big parameterlist\n",
    "        self.TT = torch.nn.ParameterList()\n",
    "        \n",
    "        # Make s instances for every mode of the tensor, or make a 3D tensor instead:\n",
    "        for n in range(self.N):\n",
    "            # Make tensors of size (r_{k-1}, n_{k} = self.s, r_{k})\n",
    "            TTcore = torch.empty(self.ranklist[n], self.s, self.ranklist[n+1])\n",
    "            torch.nn.init.xavier_normal_(TTcore)\n",
    "            self.TT.append(torch.nn.Parameter(TTcore))\n",
    "                        \n",
    "        if verbose != 0:\n",
    "            print(\"self.ranklist =\", self.ranklist)\n",
    "            print(\"self.N =\", self.N, \"+ 1 ?=\", len(self.ranklist), \"= len(self.ranklist)\") \n",
    "            print(\"TT has\", len(self.TT), \"elements\")\n",
    "        \n",
    "    def parallelVecProd(self, index):\n",
    "        self.V[index] = self.z @ self.TT[index]\n",
    "        return\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Compute the forward pass: the nmode multiplications f = A x1 z x2 z x3 ··· x(N-1) z\n",
    "        # Follow algorithm 1: allocate space and compute each V^(k), possible in parallel with threads\n",
    "        # Problem: this algorithm is mae for a scalar output. Here we will perform all but up to the last \n",
    "        # multiplication, so that V[-1] is of the required length s\n",
    "        self.z = z\n",
    "        self.V = [None] * self.N\n",
    "        # Perform parallel computation of the products: tremendous speedup\n",
    "        threads = []\n",
    "        for k in range(self.N - 1):\n",
    "            # V[k] = z @ self.TT[k]\n",
    "            # V[k] = self.parallelVecProd(z, self.TT[k])\n",
    "            process = threading.Thread(target=self.parallelVecProd, args=(k,))\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "        self.V[-1] = self.TT[k + 1][:, :, 0]\n",
    "            \n",
    "        # Wait for first thread to finish:\n",
    "        threads[0].join()\n",
    "        f = self.V[0]\n",
    "        for k in range(1, self.N):\n",
    "            threads[k].join() if k != self.N - 1 else None\n",
    "            f @= self.V[k]\n",
    "        # Now we have a vector f of size s\n",
    "        return f.reshape(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PolyGAN CP decomposition\n",
    "Here we implement the basic CP for the polyGAN polynomial approximation, Eq (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyGAN_CP_Layer(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight, verbose = 0):\n",
    "        super(PolyGAN_CP_Layer, self).__init__()\n",
    "        \n",
    "        # N = order of the polynomial = order of the tensor A:\n",
    "        # A is of dimension (s, s, ..., s) = (N x s)\n",
    "        # rank = rank used for the tensor cores\n",
    "        # size = length of the flattened input\n",
    "        self.N = N\n",
    "        self.rank = rank\n",
    "        self.s = imwidth * imheight\n",
    "        \n",
    "        # bias and weights\n",
    "        b = torch.empty(self.s)\n",
    "        torch.nn.init.xavier_normal_(b)\n",
    "        self.b = torch.nn.Parameter(b)\n",
    "        \n",
    "        self.W = torch.nn.ParameterList()\n",
    "        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Compute the forward pass: the nmode multiplications f = self.b + self.b + self.W[0]*z + z.T*...\n",
    "       \n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PolyGAN TT decomposition\n",
    "Here we implement the basic TT for the polyGAN polynomial approximation, Eq (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyGAN_TT_Layer(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight, verbose = 0):\n",
    "        super(PolyGAN_TT_Layer, self).__init__()\n",
    "        \n",
    "        # N = order of the polynomial = order of the tensor A:\n",
    "        # A is of dimension (s, s, ..., s) = (N x s)\n",
    "        # rank = rank used for the tensor cores\n",
    "        # size = length of the flattened input\n",
    "        self.N = N\n",
    "        self.rank = rank\n",
    "        self.s = imwidth * imheight\n",
    "        \n",
    "        # bias and weights\n",
    "        b = torch.empty(self.s)\n",
    "        torch.nn.init.xavier_normal_(b)\n",
    "        self.b = torch.nn.Parameter(b)\n",
    "        \n",
    "        self.W = torch.nn.ParameterList()\n",
    "        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Compute the forward pass: the nmode multiplications f = self.b + self.b + self.W[0]*z + z.T*...\n",
    "       \n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug and play\n",
    "Test the different layers in a standard net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.ranklist = [1, 3, 3, 3, 3, 1]\n",
      "self.N = 5 + 1 ?= 6 = len(self.ranklist)\n",
      "TT has 5 elements\n",
      "\n",
      "output has shape torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "# define testnetwork\n",
    "class testGenerator(torch.nn.Module):\n",
    "    def __init__(self, N, rank, imwidth, imheight):\n",
    "        super(testGenerator, self).__init__()\n",
    "        \n",
    "        self.imwidth, self.imheight = imwidth, imheight\n",
    "        self.s = imwidth*imheight\n",
    "        self.PolyLayer = FTT_Layer(N, rank, imwidth, imheight, 1)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # UserWarning: Bi-quadratic interpolation behavior has changed due to a bug in the implementation of scikit-image\n",
    "        # Perhaps another bilinear interpolation method?\n",
    "        x = skimage.transform.resize(x, (self.imwidth, self.imheight), order=1, anti_aliasing=True)\n",
    "        x = torch.tensor(x).float() # make tensor, no need for the very high precision\n",
    "        x = x.reshape(self.s) # flatten to the 1D equivalent vector\n",
    "        \n",
    "        x = self.PolyLayer(x)\n",
    "        x = x.reshape(self.imwidth, self.imheight)\n",
    "        return x\n",
    "\n",
    "net = testGenerator(N, rank, imwidth, imheight)\n",
    "\n",
    "output = net(low_res_sample)\n",
    "print(\"\\noutput has shape\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  5.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Time the learning process:\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loops = 10\n",
    "\n",
    "net.train()\n",
    "\n",
    "pred_timer = []\n",
    "crit_timer = []\n",
    "optm_timer = []\n",
    "loss_timer = []\n",
    "step_timer = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for t in tqdm(range(loops)):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    pred_start = time.time()\n",
    "    pred = net(low_res_sample)\n",
    "    pred_timer.append(time.time() - pred_start)\n",
    "    \n",
    "    crit_start = time.time()\n",
    "    loss = criterion(pred, high_res_sample)\n",
    "    crit_timer.append(time.time() - crit_start)\n",
    "    \n",
    "    optm_start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    optm_timer.append(time.time() - optm_start)\n",
    "    \n",
    "    loss_start = time.time()\n",
    "    loss.backward()\n",
    "    loss_timer.append(time.time() - loss_start)\n",
    "    \n",
    "    step_start = time.time()\n",
    "    optimizer.step()\n",
    "    step_timer.append(time.time() - step_start)\n",
    "\n",
    "total_elapsed = time.time() - start\n",
    "elapsed_per_loop = float(total_elapsed/loops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time = 2.018371105194092 s, with 0.20183711051940917 s per loop\n",
      "pred_timer = 0.03944780826568604 s on average\n",
      "crit_timer = 0.0016765117645263672 s on average\n",
      "optm_timer = 0.004449605941772461 s on average\n",
      "loss_timer = 0.08905975818634033 s on average\n",
      "step_timer = 0.06591477394104003 s on average\n"
     ]
    }
   ],
   "source": [
    "mean = lambda x : sum(x)/len(x)\n",
    "\n",
    "print(\"Total elapsed time =\", total_elapsed, \"s, with\", elapsed_per_loop, \"s per loop\")\n",
    "print(\"pred_timer =\", mean(pred_timer), \"s on average\")\n",
    "print(\"crit_timer =\", mean(crit_timer), \"s on average\")\n",
    "print(\"optm_timer =\", mean(optm_timer), \"s on average\")\n",
    "print(\"loss_timer =\", mean(loss_timer), \"s on average\")\n",
    "print(\"step_timer =\", mean(step_timer), \"s on average\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
