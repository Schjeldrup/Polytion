{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring tensor decompositions in pytorch\n",
    "Simon Aertssen\n",
    "17/03/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an exploration of the different functions and methods necessary to perform the CANDECOMP/PARAFAC and Tensor-Train Decompositions. These will be gathered in a ```polution``` package later. The ```tensorly```package is a great resource: https://github.com/tensorly/tensorly/tree/master/tensorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold/ Unfold \n",
    "https://github.com/tensorly/tensorly/blob/c1a3051f43b11350ee29c28cc81ec2cd2c60e99c/tensorly/base.py#L54\n",
    "\n",
    "MoveAxis method\n",
    "https://github.com/tensorly/tensorly/blob/0a89edd824431f66d5a76c1a3e097a80c8475978/tensorly/backend/pytorch_backend.py#L90\n",
    "\n",
    "Parafac:\n",
    "https://github.com/tensorly/tensorly/blob/master/tensorly/decomposition/candecomp_parafac.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Useful products:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{Matrix Kronecker product}$ is defined as\n",
    "\n",
    "\\begin{align*} \\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}} \\otimes \\boldsymbol{Q}^{\\:\\mathrm{K} \\times \\mathrm{L}}=\\boldsymbol{R}^{\\:\\mathrm{IK} \\times \\mathrm{JL}}, \\text { such that } r_{k+K(i-1), \\: l+L(j-1)}=p_{i j} q_{k l} \\end{align*}\n",
    "\n",
    "Naive: use loops. Smart: reshape P and Q so that their dimensions do not overlap, then the elementwise or Hadamard product gives the desired result.\n",
    "\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product) for the examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KronProd2D(P, Q):\n",
    "    # Register dimensions.\n",
    "    I, J = P.shape\n",
    "    K, L = Q.shape\n",
    "    \n",
    "    # Adjust dimensions of P and Q to perform smart multiplication:\n",
    "    # interweave the dimensions containing values and perform elementwise multiplication.\n",
    "    P = P.view(I, 1, J, 1)\n",
    "    Q = Q.view(1, K, 1, L)\n",
    "    \n",
    "    R = P * Q\n",
    "    return R.view(I*K, J*L)\n",
    "\n",
    "def KronProd(P, Q):\n",
    "    # This should work for higher order tensors.\n",
    "    # Register and check dimensions.\n",
    "    pshape = P.shape\n",
    "    qshape = Q.shape\n",
    "    if P.dim() != Q.dim():\n",
    "        raise ValueError('Matrices should be of the same order: P.dim() =' + str(P.dim()) + ' != Q.dim() = ' + str(Q.dim()))\n",
    "    \n",
    "    # Adjust dimensions of P and Q to perform smart multiplication:\n",
    "    # interweave the dimensions containing values and perform elementwise multiplication.\n",
    "    # Start with a list of ones and set dimensions as every even or uneven index.\n",
    "    pindices = [1]*2*len(pshape)    \n",
    "    pindices[::2] = pshape\n",
    "    qindices = [1]*2*len(qshape)\n",
    "    qindices[1::2] = qshape\n",
    "        \n",
    "    P = P.view(pindices)\n",
    "    Q = Q.view(qindices)\n",
    "    \n",
    "    R = P * Q\n",
    "    rshape = [p*q for p, q in zip(pshape,qshape)]\n",
    "    return R.view(rshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.tensor([[1, -4, 7], [-2, 3, 3]])\n",
    "Q = torch.tensor([[8, -9, -6, 5], [1, -3, -4, 7], [2, 8, -8, -3], [1, 2, -5, -1]])\n",
    "R = KronProd(P, Q)\n",
    "assert torch.all(R.eq(KronProd2D(P, Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textbf{Kahtri-Rhao product}$ is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}}|\\otimes| \\boldsymbol{Q}^{\\mathrm{K} \\times \\mathrm{J}}=\\boldsymbol{P}^{\\:\\mathrm{I} \\times \\mathrm{J}} \\odot \\boldsymbol{Q}^{\\mathrm{K} \\times \\mathrm{J}}=\\boldsymbol{R}^{\\:\\mathrm{IK} \\times \\mathrm{J}}, \\text { such that } r_{k+K(i-1), j}=p_{i j} q_{k j}\n",
    "\\end{align*}\n",
    "\n",
    "This can be seen as a column-wise kronecker product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KathRaoProd(P, Q):\n",
    "    # Register and check dimensions\n",
    "    pshape = P.shape\n",
    "    qshape = Q.shape\n",
    "    J = pshape[-1]\n",
    "    if J != qshape[-1]:\n",
    "        raise ValueError('Matrices should have the same number of columns: ' + str(J) + ' != ' + str(qshape[-1]))\n",
    "    \n",
    "    # Make R an empty tensor\n",
    "    rshape = [p*q for p, q in zip(pshape,qshape)]\n",
    "    rshape[-1] = J\n",
    "    R = torch.zeros(rshape)\n",
    "    for j in range(J):\n",
    "        R[:,j] = KronProd(P[:,j], Q[:,j])\n",
    "                \n",
    "    # Tried with slicing but did not seem to work:\n",
    "    # column_indices = list(range(J)) or slice(0,J)\n",
    "    # R[:, column_indices] = KronProd(P[:, column_indices], Q[:, column_indices])\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "Q = torch.tensor([[1, 4, 7], [2, 5, 8], [3, 6, 9]])\n",
    "R = KathRaoProd(P, Q)\n",
    "#print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mode-m Matricization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the decompositions we will need tensor unfolding. The mode-$n$ matricization of a tensor $\\mathcal{X} \\in \\mathrm{R}^{I_{1} \\times I_{2} \\times \\ldots \\times I_{N}}$ is denoted $\\boldsymbol{X}_{(n)}$ and arranges the mode-$n$ fibres to be the columns of the resulting matrix.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{X}^{I_{1} \\times I_{2} \\times \\ldots \\times I_{N}} \\hspace{5mm}\n",
    "\\overset{\\underset{\\text{unfolding}}{\\longrightarrow}}{\\underset{\\text{folding}}{\\longleftarrow}} \\hspace{5mm}\n",
    "\\boldsymbol{X}_{(n)}^{I_{n} \\times I_{1} \\cdot I_{2} \\cdots I_{n-1} \\cdot I_{n+1} \\cdots I_{N}} \n",
    "\\end{align*}\n",
    "\n",
    "While writing the function underneath (```Unfold```) it became clear that Kolda's definition was lacking some of the intuition behind torch tensors and their indexing. A good discussion can be found here: https://jeankossaifi.com/blog/unfolding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Permute_tensor(tensor, origin, axis, outputshape=None):\n",
    "    # Perform the permutation of the tensor here, to be used in both Fold and Unfold.\n",
    "    # For more explanation on this function see Unfold().\n",
    "    \n",
    "    dims = list(range(tensor.dim()))\n",
    "    # For later use, we want to be able to map indices for a circular permutation\n",
    "    if origin < 0: origin = dims[origin]\n",
    "    if axis < 0: axis = dims[axis]\n",
    "            \n",
    "    dims.pop(origin)\n",
    "    dims = dims[::-1]\n",
    "    dims.insert(axis, origin)\n",
    "    tensor = tensor.permute(*dims)\n",
    "    \n",
    "    if outputshape:\n",
    "        tensor = tensor.reshape(outputshape)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def Unfold(tensor, mode):\n",
    "    # Notation sets minimal mode to be 1, but we want to map that to zero. We also cannot mode_n unfold a matrix with n-1 modes.\n",
    "    if mode > len(tensor.shape) or mode < 0:\n",
    "        raise ValueError('Tensor has order ' + str(len(tensor.shape)) + ', cannot give mode ' + str(mode))\n",
    "    \n",
    "    # Register tensor shape and mode size\n",
    "    order = tensor.dim()\n",
    "    tshape = tensor.shape    \n",
    "    I_n = tshape[mode]\n",
    "    \n",
    "    # To get the prescribed matrix in Kolda et al, we need to perform a permutation of the axes.\n",
    "    # The selected mode needs to be viewed as the first, and all other dimensions should reverse their order.\n",
    "    # It took a lot of experiments to get this right, the full functionality has been moved to Permute_tensor().    \n",
    "    return Permute_tensor(tensor, mode, 0, (I_n, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([3, 4, 2])\n",
      "X_1 = \n",
      "[[ 1  4  7 10]\n",
      " [ 2  5  8 11]\n",
      " [ 3  6  9 12]]\n",
      "X_2 = \n",
      "[[13 16 19 22]\n",
      " [14 17 20 23]\n",
      " [15 18 21 24]]\n",
      "X = \n",
      "[[[ 1 13]\n",
      "  [ 4 16]\n",
      "  [ 7 19]\n",
      "  [10 22]]\n",
      "\n",
      " [[ 2 14]\n",
      "  [ 5 17]\n",
      "  [ 8 20]\n",
      "  [11 23]]\n",
      "\n",
      " [[ 3 15]\n",
      "  [ 6 18]\n",
      "  [ 9 21]\n",
      "  [12 24]]]\n"
     ]
    }
   ],
   "source": [
    "# Following the example on p460:\n",
    "# This definition of X gives wrong dimensions:\n",
    "#X = torch.tensor([[ [1, 4, 7, 10], [2, 5, 8, 11], [3, 6, 9, 12] ], [ [13, 16, 19, 22], [14, 17, 20, 23], [15, 18, 21, 24] ]])\n",
    "\n",
    "X = torch.tensor([[ [1,13], [4,16], [7,19], [10,22] ], [ [2,14], [5,17], [8,20], [11,23] ], [ [3,15], [6,18], [9,21], [12,24] ]])\n",
    "print(\"X.shape = \" + str(X.shape))\n",
    "X1 = X[...,0]\n",
    "X2 = X[...,1]\n",
    "print(\"X_1 = \\n\" + str(X1.numpy()))\n",
    "print(\"X_2 = \\n\" + str(X2.numpy()))\n",
    "print(\"X = \\n\" + str(X.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  7 10 13 16 19 22]\n",
      " [ 2  5  8 11 14 17 20 23]\n",
      " [ 3  6  9 12 15 18 21 24]]\n",
      "[[ 1  2  3 13 14 15]\n",
      " [ 4  5  6 16 17 18]\n",
      " [ 7  8  9 19 20 21]\n",
      " [10 11 12 22 23 24]]\n",
      "[[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      " [13 14 15 16 17 18 19 20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "print(Unfold(X, 0).numpy())\n",
    "print(Unfold(X, 1).numpy())\n",
    "print(Unfold(X, 2).numpy())\n",
    "# Note that the .numpy() is only here to represent the unfolded tensors in a nice format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need a ```Fold``` function? Update: yes. Perform reverse operations of ```Unfold```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fold(unfolded_tensor, mode, desired_shape):\n",
    "    # Perform the same entry safety controls\n",
    "    if mode > len(desired_shape) or mode < 0:\n",
    "        raise ValueError('Folded tensor has order ' + str(len(desired_shape)) + ', cannot give mode ' + str(mode))\n",
    "    \n",
    "    # Before calling Permute_tensor(), we need to reshape the unfolded tensor back to its' desired order.\n",
    "    # This does not yield the desired tensor but takes into account the coming permutation.\n",
    "    shape = list(desired_shape)\n",
    "    axis = shape.pop(mode)\n",
    "    shape = shape[::-1]\n",
    "    shape.insert(0, axis)\n",
    "    tensor = unfolded_tensor.reshape(shape)\n",
    "    \n",
    "    # return Permute_tensor(tensor, 0, mode, desired_shape)\n",
    "    # Exactly my point before, we have accounted for the exact inverse transformation without the need for a reshape.\n",
    "    # Both return statements yield the same result.\n",
    "    return Permute_tensor(tensor, 0, mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FoldedX_ 0 =\n",
      " [[[ 1 13]\n",
      "  [ 4 16]\n",
      "  [ 7 19]\n",
      "  [10 22]]\n",
      "\n",
      " [[ 2 14]\n",
      "  [ 5 17]\n",
      "  [ 8 20]\n",
      "  [11 23]]\n",
      "\n",
      " [[ 3 15]\n",
      "  [ 6 18]\n",
      "  [ 9 21]\n",
      "  [12 24]]]\n",
      "FoldedX_ 1 =\n",
      " [[[ 1 13]\n",
      "  [ 4 16]\n",
      "  [ 7 19]\n",
      "  [10 22]]\n",
      "\n",
      " [[ 2 14]\n",
      "  [ 5 17]\n",
      "  [ 8 20]\n",
      "  [11 23]]\n",
      "\n",
      " [[ 3 15]\n",
      "  [ 6 18]\n",
      "  [ 9 21]\n",
      "  [12 24]]]\n",
      "FoldedX_ 2 =\n",
      " [[[ 1 13]\n",
      "  [ 4 16]\n",
      "  [ 7 19]\n",
      "  [10 22]]\n",
      "\n",
      " [[ 2 14]\n",
      "  [ 5 17]\n",
      "  [ 8 20]\n",
      "  [11 23]]\n",
      "\n",
      " [[ 3 15]\n",
      "  [ 6 18]\n",
      "  [ 9 21]\n",
      "  [12 24]]]\n"
     ]
    }
   ],
   "source": [
    "for mode in range(3):\n",
    "    FoldedX = Fold(Unfold(X, mode), mode, X.shape)\n",
    "    print(\"FoldedX_\", mode, \"=\\n\", FoldedX.numpy())\n",
    "    assert torch.all(X.eq(FoldedX))\n",
    "# If no errors, all are exactly equal to X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CP decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the alternating least squares approach. Start with random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPD(tensor, rank=5, maxiter=100):\n",
    "    # Problems were recorded with tensors that had only type = torch.LongTensor.\n",
    "    # Reformat into torch.FloatTensor, and clone to avoid performing CPD twice on the same tensor.\n",
    "    tensor = tensor.type(torch.FloatTensor).clone()\n",
    "    \n",
    "    # Note that we will perform n_iter_max iterations per tensor order!\n",
    "    # Register input variables:\n",
    "    tshape = tensor.shape\n",
    "    order = tensor.dim()\n",
    "    \n",
    "    # Initialization: create a list of matrices to represent the input tensor: tensor = [t_1, t_2, .. , t_order]\n",
    "    # Incorrect: ABC = [torch.rand((rank, rank))]*order\n",
    "    ABC = []\n",
    "    for o in range(order):\n",
    "        ABC.append(torch.rand((tshape[o], rank)))\n",
    "    \n",
    "    # Main loop: for every iteration, adjust every factor matrix\n",
    "    for iteration in range(maxiter):\n",
    "        for i, fmatrix in enumerate(ABC):\n",
    "            # Get all the other fmatrices, temporarily remove the element:\n",
    "            ABC.pop(i)\n",
    "\n",
    "            # Construct V: use the first element ABC[0] and the loop over everything after with ABC[1::]\n",
    "            V = ABC[0].t() @ ABC[0]\n",
    "            # pinverse_V = torch.ones((rank, rank), dtype=tensor.dtype)\n",
    "            for ABC_notfmatrix in ABC[1::]:\n",
    "                V = V * (ABC_notfmatrix.t() @ ABC_notfmatrix)\n",
    "                # pinverse_V = pinverse_V * (fmatrix.t() @ fmatrix)\n",
    "            \n",
    "            # Construct W (kr product of all matrices other way around):\n",
    "            # take the last element of ABC and then loop over ABC[::-1] excluding the last element.\n",
    "            # Example: ABC = [1,2,3,4] and ABC[::-1][1::] = [3,2,1] and ABC[1::-1] = [2,1]\n",
    "            # This would be easier with a pop and insert, but we need to recycle ABC a lot...\n",
    "            W = ABC[-1]\n",
    "            for ABC_notfmatrix in ABC[::-1][1::]:\n",
    "                W = KathRaoProd(W, ABC_notfmatrix)\n",
    "                \n",
    "            # Here is a problem:\n",
    "            fmatrix = Unfold(tensor, i) @ W @ torch.pinverse(V)\n",
    "            # Other tries to get the p_inverse\n",
    "            # pinverse_V = ((V.t() @ V).inverse()) @ V.t()\n",
    "            # pinverse_V = V.t() @ ((V.t() @ V).inverse()) \n",
    "            # fmatrix = Unfold(tensor, i) @ W @ pinverse_V\n",
    "            \n",
    "            # Push the lost fmatrix back in:\n",
    "            ABC.insert(i, fmatrix)\n",
    "    return ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "3 factor matrices\n",
      "torch.Size([10, 5])\n",
      "torch.Size([15, 5])\n",
      "torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "# test:\n",
    "tensor = torch.rand((10,15,20))\n",
    "print(tensor.type())\n",
    "factor_matrices = CPD(tensor)\n",
    "print(len(factor_matrices), \"factor matrices\")\n",
    "\n",
    "for fm in factor_matrices:\n",
    "    print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnfoldFM(factor_matrices, mode):\n",
    "    if mode > len(factor_matrices) or mode < 0:\n",
    "        raise ValueError('Tensor has order ' + str(len(factor_matrices)) + ', cannot give mode ' + str(mode))\n",
    "    # Start with the first element (for normal dot product), then the last element is ready for the backward loop.\n",
    "    first = factor_matrices.pop(mode)\n",
    "    prod = factor_matrices.pop(-1)\n",
    "    for factor in factor_matrices[::-1]:\n",
    "        prod = KathRaoProd(prod, factor)\n",
    "    return first @ prod.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9602,  3.9909,  7.0215, 10.0522, 13.2199, 16.0837, 18.9474, 21.8112],\n",
      "        [ 2.0001,  5.0003,  8.0005, 11.0006, 13.9877, 16.9947, 20.0018, 23.0088],\n",
      "        [ 3.0399,  6.0097,  8.9794, 11.9491, 14.7554, 17.9058, 21.0561, 24.2065]])\n",
      "tensor([[ 1,  4,  7, 10, 13, 16, 19, 22],\n",
      "        [ 2,  5,  8, 11, 14, 17, 20, 23],\n",
      "        [ 3,  6,  9, 12, 15, 18, 21, 24]])\n"
     ]
    }
   ],
   "source": [
    "factor_matrices = CPD(X, 2)\n",
    "print(UnfoldFM(factor_matrices, 0))\n",
    "print(Unfold(X, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compose(factors):\n",
    "    # Make copy to remove dependancy\n",
    "    factor_matrices = []\n",
    "    for f in factors:\n",
    "        factor_matrices.append(f.clone())\n",
    "    \n",
    "    # Register dimensions of the tensor:\n",
    "    order = len(factor_matrices)\n",
    "    tshape = [x.shape[0] for x in factor_matrices]\n",
    "    \n",
    "    # Use the mode-0 unfolding and just fold it back.\n",
    "    return Fold(UnfoldFM(factor_matrices, 0), 0, tshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.7990)\n",
      "tensor(31.7990)\n",
      "tensor(0.0005)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((10,15,20))\n",
    "factor_matrices = CPD(tensor, 100)\n",
    "test = Compose(factor_matrices)\n",
    "\n",
    "print(torch.norm(tensor))\n",
    "print(torch.norm(test))\n",
    "print(torch.norm(tensor - test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor has 3000 elements.\n",
      "The factor matrices have 3000000000 elements.\n"
     ]
    }
   ],
   "source": [
    "lmtns = tensor.shape\n",
    "print(\"The tensor has\", lmtns[0]*lmtns[1]*lmtns[2], \"elements.\")\n",
    "\n",
    "fmatrixdims = 1\n",
    "for f in factor_matrices:\n",
    "    fmatrixdims *= f.shape[0]*f.shape[1]\n",
    "    \n",
    "print(\"The factor matrices have\", fmatrixdims, \"elements.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
